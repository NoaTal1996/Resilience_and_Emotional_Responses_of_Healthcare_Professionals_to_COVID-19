{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8df5967-f554-48b1-a7bf-5700e1ea24e5",
      "metadata": {
        "id": "f8df5967-f554-48b1-a7bf-5700e1ea24e5"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb5b6a0-e9ab-4bd7-bd5f-e14adbcb41ea",
      "metadata": {
        "id": "3bb5b6a0-e9ab-4bd7-bd5f-e14adbcb41ea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.seasonal import STL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30eb287b-7931-4bc4-a419-423dc617e23d",
      "metadata": {
        "id": "30eb287b-7931-4bc4-a419-423dc617e23d"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fafc04-2311-4dab-b98e-774320058777",
      "metadata": {
        "id": "e1fafc04-2311-4dab-b98e-774320058777"
      },
      "outputs": [],
      "source": [
        "def convert_date(date_str):\n",
        "    try:\n",
        "        date_obj = datetime.strptime(str(date_str), '%a %b %d %H:%M:%S %z %Y')\n",
        "        return date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "def convert_date_df(df):\n",
        "    df['date'] = df['created_at'].apply(convert_date)\n",
        "    df = df.dropna(subset=['date'])\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # Create 'year' and 'month' columns\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_emotion(df):\n",
        "    df = df[['screen_name','joy_pys','sadness_pys','anger_pys','surprise_pys','disgust_pys','fear_pys','date']]\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    return df\n",
        "\n",
        "def filter_emotion_all(df):\n",
        "    df = df[['screen_name','joy','sadness','anger','surprise','disgust','fear','date']]\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    return df\n",
        "def avg_emotion_per_user(df):\n",
        "    df.loc[:, 'date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    df.set_index('date', inplace=True)\n",
        "    # Resample DataFrame to weekly frequency and calculate mean for each user\n",
        "    weekly_avg_per_user = df.groupby('screen_name').resample('M').mean()#w\n",
        "    # print(weekly_avg_per_user)\n",
        "\n",
        "    # Calculate the overall weekly average by taking the mean of the weekly averages per user\n",
        "    # overall_weekly_avg = weekly_avg_per_user.mean(level='date')\n",
        "    overall_weekly_avg = weekly_avg_per_user.groupby(level='date').mean()\n",
        "\n",
        "    return overall_weekly_avg\n",
        "\n",
        "\n",
        "def replace_column_emotion(df):\n",
        "    df.columns = df.columns.str.replace('_pys', '', regex=False)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbfa62e-3409-4711-852b-db59c40f9c98",
      "metadata": {
        "id": "dbbfa62e-3409-4711-852b-db59c40f9c98"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3496fec-cd80-4d4b-82c8-90de02589191",
      "metadata": {
        "id": "a3496fec-cd80-4d4b-82c8-90de02589191"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data_cross/owid-covid-data.csv')\n",
        "usa = df[df['iso_code'] == \"USA\"]\n",
        "usa_filtered = usa\n",
        "#option to create the left graph\n",
        "usa_filtered = usa[(usa['hosp_patients'] > 0) & (usa['new_cases'] > 0) & (usa['new_deaths'] > 0)]\n",
        "usa_filtered = usa.dropna(subset=['hosp_patients', 'new_cases', 'new_deaths'])\n",
        "usa_filtered = usa.dropna(subset=[ 'new_cases'])\n",
        "start_date = '2020-01-26'\n",
        "usa_filtered = usa_filtered[usa_filtered['date'] >= start_date]\n",
        "usa_filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a05398",
      "metadata": {
        "id": "50a05398"
      },
      "outputs": [],
      "source": [
        "max_date = '2022-05-15 00:00:00'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42a85b8-cdd1-49d5-80a3-6943090a60bd",
      "metadata": {
        "id": "c42a85b8-cdd1-49d5-80a3-6943090a60bd"
      },
      "source": [
        "# Functions to create graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e31a51-29d0-4f54-99b6-c0e3ff6f0770",
      "metadata": {
        "id": "07e31a51-29d0-4f54-99b6-c0e3ff6f0770"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "def pipline_analysis(final_h,final_df,usa,path, time_d,lags,divides_std,trend):\n",
        "    if not os.path.exists(path):\n",
        "        # If the directory does not exist, create it\n",
        "        os.makedirs(path)\n",
        "        print(\"Directory created:\", path)\n",
        "\n",
        "\n",
        "    hcp_avg_user = final_h.groupby('screen_name').resample(time_d).mean(numeric_only=True) # !!!!# Convert to comment\n",
        "    general_avg_user = final_df.groupby('screen_name').resample(time_d).mean(numeric_only=True)# Convert to comment\n",
        "    hcp_avg_user = hcp_avg_user.reset_index()# Convert to comment\n",
        "    general_avg_user = general_avg_user.reset_index()# Convert to comment\n",
        "\n",
        "    usa_copy = usa.copy()\n",
        "    # usa_1 = usa_copy.dropna(subset=[\"hosp_patients\"])\n",
        "    usa_1 = usa_copy.dropna(subset=[\"new_cases\"])\n",
        "\n",
        "    usa_1['date'] = pd.to_datetime(usa_1['date'])\n",
        "    min_date = usa_1['date'].min()\n",
        "\n",
        "    overall_weekly_stats_h,max_date = calculate_df_per_tweet_limit(hcp_avg_user,min_date,time_d,divides_std,trend)# Convert to comment\n",
        "    overall_weekly_stats_g,max_date = calculate_df_per_tweet_limit(general_avg_user,min_date, time_d,divides_std,trend)#0.30, 0.20 # Convert to comment\n",
        "    # return overall_weekly_stats_h,max_date,overall_weekly_stats_g,max_date\n",
        "\n",
        "    excel_corr_populations(usa, overall_weekly_stats_h, overall_weekly_stats_g, min_date, max_date,time_d,path,lags,divides_std,trend)\n",
        "    excel_corr_data(usa,min_date, max_date,time_d,path,lags,divides_std,trend)\n",
        "\n",
        "    path_img = path +\"/correletio_between emotion_to_m\"\n",
        "    if not os.path.exists(path_img):\n",
        "        # If the directory does not exist, create it\n",
        "        os.makedirs(path_img)\n",
        "    measures = ['new_cases', 'new_deaths','hosp_patients']#'hosp_patients_smoothed',\n",
        "    for m in measures:\n",
        "        analyze_correlations_between_em_me(usa, overall_weekly_stats_h, overall_weekly_stats_g, m, path_img,min_date,max_date,time_d,lags,divides_std,trend)\n",
        "\n",
        "    emotions = ['joy', 'sadness', 'anger', 'surprise', 'disgust', 'fear']\n",
        "    path_img = path +\"/real_emotion_to_m\"\n",
        "    if not os.path.exists(path_img):\n",
        "        # If the directory does not exist, create it\n",
        "        os.makedirs(path_img)\n",
        "    for m in measures:\n",
        "        for emotion in emotions:\n",
        "\n",
        "            graph_norm(overall_weekly_stats_h,overall_weekly_stats_g, m,emotion,time_d,path_img,min_date,max_date,divides_std,trend,usa)\n",
        "            graph_norm_1(overall_weekly_stats_h,overall_weekly_stats_g, m,emotion,time_d,path_img,min_date,max_date,divides_std,trend,usa)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9283ccb1-e765-416d-8d41-64b03d8642c1",
      "metadata": {
        "id": "9283ccb1-e765-416d-8d41-64b03d8642c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import t\n",
        "\n",
        "def calculate_df_per_tweet_limit(df,min_date, grouped,divides_std,trend):\n",
        "    # print(min_date)\n",
        "    max_date = df['date'].max() - pd.Timedelta(days=21)\n",
        "    df = df[(df['date'] >= min_date) & (df['date'] <= max_date)]\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df.set_index('date', inplace=True)\n",
        "\n",
        "    # Select only numeric columns for quantile calculation\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    overall_weekly_avg = numeric_df.resample(grouped).mean()\n",
        "    overall_weekly_std = numeric_df.resample(grouped).std()\n",
        "    overall_weekly_count = numeric_df.resample(grouped).count()\n",
        "\n",
        "    # If divides_std is True, divide each column's mean by its std\n",
        "    if divides_std:\n",
        "        for col in numeric_df.columns:\n",
        "            overall_weekly_avg[col] = overall_weekly_avg[col] / overall_weekly_std[col]\n",
        "\n",
        "    # Rename the columns to reflect that these are averages possibly divided by std\n",
        "    overall_weekly_avg.columns = ['avg_' + col for col in overall_weekly_avg.columns]\n",
        "\n",
        "\n",
        "    # Concatenate the mean and confidence interval dataframes\n",
        "    overall_weekly_stats = pd.concat([overall_weekly_avg], axis=1)\n",
        "\n",
        "    return overall_weekly_stats,max_date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12689593-238f-4463-b644-2b80fe2aed15",
      "metadata": {
        "id": "12689593-238f-4463-b644-2b80fe2aed15"
      },
      "outputs": [],
      "source": [
        "def excel_corr_populations(usa, overall_weekly_stats_h, overall_weekly_stats_g, min_date, max_date,time_d,path,lags,divides_std,trend):\n",
        "    measures = ['new_cases','hosp_patients','new_deaths']#'hosp_patients_smoothed',\n",
        "\n",
        "    # Initialize list to store results\n",
        "    table_data = []\n",
        "\n",
        "    for measure in measures:\n",
        "\n",
        "        # Analyze correlations\n",
        "        result = analyze_correlations(usa, overall_weekly_stats_h, overall_weekly_stats_g, measure, min_date, max_date,time_d,lags,divides_std,trend)\n",
        "\n",
        "        # Append results for healthcare professionals and non-healthcare professionals\n",
        "        for idx, emotion in enumerate(result['hcp']['emotion']):\n",
        "            table_data.append({\n",
        "                'hcp/emotion': f'hcp-{emotion[4:]}',  # Strip 'avg_' from emotion name\n",
        "                'emotion': emotion[4:],  # Add emotion name for sorting\n",
        "                'measure': measure,\n",
        "                'lag': result['hcp']['lag'][idx],\n",
        "                'significance': result['hcp']['significance'][idx],\n",
        "                'corr': result['hcp']['corr'][idx]\n",
        "            })\n",
        "            table_data.append({\n",
        "                'hcp/emotion': f'non-hcp-{emotion[4:]}',  # Strip 'avg_' from emotion name\n",
        "                'emotion': emotion[4:],  # Add emotion name for sorting\n",
        "                'measure': measure,\n",
        "                'lag': result['non_hcp']['lag'][idx],\n",
        "                'significance': result['non_hcp']['significance'][idx],\n",
        "                'corr': result['non_hcp']['corr'][idx]\n",
        "            })\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(table_data)\n",
        "    results_df.to_csv(path+\"/results_corr_finish.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b37b87e-64b8-4ab0-97da-e5cf832130c9",
      "metadata": {
        "id": "5b37b87e-64b8-4ab0-97da-e5cf832130c9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def analyze_correlations(usa, emotion_data_h, emotion_data_g, measure_column, min_date, max_date,group_d,lags,divied_s,trend):\n",
        "    emotion_data_h.index = pd.to_datetime(emotion_data_h.index)\n",
        "    emotion_data_g.index = pd.to_datetime(emotion_data_g.index)\n",
        "\n",
        "    # Select the relevant columns (usa,measure_column, min_date, max_date,divied_s,trend,group_d)\n",
        "    weekly_avg_daily_confirmed_cases = clean_usa(usa,measure_column, min_date, max_date,divied_s,trend,group_d)[measure_column]\n",
        "\n",
        "    aggregate_emotions_h_df = emotion_data_h[['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']]\n",
        "    aggregate_emotions_g_df = emotion_data_g[['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']]\n",
        "\n",
        "    # Align the datasets to ensure they have the same length and the same dates\n",
        "    aligned_data_h = pd.merge(weekly_avg_daily_confirmed_cases, aggregate_emotions_h_df, left_index=True, right_index=True, how='inner')\n",
        "    aligned_data_g = pd.merge(weekly_avg_daily_confirmed_cases, aggregate_emotions_g_df, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    # Define normalization function\n",
        "    def normalize(x, range=(0, 1)):\n",
        "        return (x - np.min(x)) / (np.max(x) - np.min(x)) * (range[1] - range[0]) + range[0]\n",
        "\n",
        "    # Define function to calculate Spearman correlation with lag\n",
        "    def spearmanr_with_lag(x, y, lag):\n",
        "        if lag > 0:\n",
        "            return spearmanr(x[:-lag], y[lag:])\n",
        "        elif lag < 0:\n",
        "            return spearmanr(x[-lag:], y[:lag])\n",
        "        else:\n",
        "            return spearmanr(x, y)\n",
        "\n",
        "    # Calculate and plot correlations for both populations on the same graph\n",
        "    emotion_list = ['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']\n",
        "    measure_name = f'weekly_avg_{measure_column}'\n",
        "\n",
        "        # Initialize dictionaries to store results\n",
        "    results_hcp = {'lag': [], 'significance': [], 'corr': [], 'emotion': []}\n",
        "    results_non_hcp = {'lag': [], 'significance': [], 'corr': [], 'emotion': []}\n",
        "\n",
        "\n",
        "\n",
        "    for emotion in emotion_list:\n",
        "        # Normalize data\n",
        "        # measure_data_h = normalize(aligned_data_h[measure_column])#######!!!!!!!!\n",
        "        # emotion_data_normalized_h = normalize(aligned_data_h[emotion])#######!!!!!!!!\n",
        "        # emotion_data_normalized_h = savgol_filter(aligned_data_h[emotion].tolist(), window_length=11, polyorder=2)\n",
        "        measure_data_h = aligned_data_h[measure_column].tolist()\n",
        "        emotion_data_normalized_h = aligned_data_h[emotion].tolist()\n",
        "\n",
        "\n",
        "        # measure_data_g = normalize(aligned_data_g[measure_column])\n",
        "        # emotion_data_normalized_g = normalize(aligned_data_g[emotion])\n",
        "\n",
        "        measure_data_g = aligned_data_g[measure_column].tolist()\n",
        "        emotion_data_normalized_g = aligned_data_g[emotion].tolist()\n",
        "        # emotion_data_normalized_g = savgol_filter(aligned_data_g[emotion].tolist(), window_length=11, polyorder=2)\n",
        "\n",
        "        if trend:\n",
        "\n",
        "            stl = STL(emotion_data_normalized_h, seasonal=21).fit() #######################3new\n",
        "            emotion_data_normalized_h = stl.trend\n",
        "\n",
        "            stl = STL(emotion_data_normalized_g, seasonal=21).fit() #######################3new\n",
        "            emotion_data_normalized_g = stl.trend\n",
        "\n",
        "      # Initialize variables to store the best correlation results\n",
        "        max_corr_h = 0  # For healthcare professionals\n",
        "        best_pvalue_h = 0\n",
        "        best_lag_h = 0\n",
        "\n",
        "        max_corr_g = 0  # For non-healthcare professionals\n",
        "        best_pvalue_g = 0\n",
        "        best_lag_g = 0\n",
        "\n",
        "        # Check correlations for different time displacements (lags)\n",
        "        for displacement in lags:\n",
        "            # Calculate Spearman correlation with lag for healthcare professionals\n",
        "            corr_h = spearmanr_with_lag(measure_data_h, emotion_data_normalized_h, displacement)\n",
        "            spearman_corr_h, p_value_h = corr_h[0], corr_h[1]\n",
        "\n",
        "            # Update best correlation if current is greater\n",
        "            if abs(max_corr_h)< abs(spearman_corr_h):\n",
        "                max_corr_h = corr_h[0]\n",
        "                best_pvalue_h = p_value_h\n",
        "                best_lag_h = displacement\n",
        "\n",
        "            # Calculate Spearman correlation with lag for non-healthcare professionals\n",
        "            corr_g = spearmanr_with_lag(measure_data_g, emotion_data_normalized_g, displacement)\n",
        "            spearman_corr_g, p_value_g = corr_g[0], corr_g[1]\n",
        "\n",
        "            # Update best correlation if current is greater\n",
        "            if abs(max_corr_g) < abs(spearman_corr_g):\n",
        "                max_corr_g = corr_g[0]\n",
        "                best_pvalue_g = p_value_g\n",
        "                best_lag_g = displacement\n",
        "\n",
        "        # Store the results for healthcare professionals\n",
        "        results_hcp['lag'].append(best_lag_h)\n",
        "        results_hcp['significance'].append(best_pvalue_h)\n",
        "        results_hcp['corr'].append(max_corr_h)\n",
        "        results_hcp['emotion'].append(emotion)\n",
        "\n",
        "        # Store the results for non-healthcare professionals\n",
        "        results_non_hcp['lag'].append(best_lag_g)\n",
        "        results_non_hcp['significance'].append(best_pvalue_g)\n",
        "        results_non_hcp['corr'].append(max_corr_g)\n",
        "        results_non_hcp['emotion'].append(emotion)\n",
        "\n",
        "    return {\n",
        "        'hcp': results_hcp,\n",
        "        'non_hcp': results_non_hcp\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6bfdf92-3634-4310-b76c-1c36a108224f",
      "metadata": {
        "id": "a6bfdf92-3634-4310-b76c-1c36a108224f"
      },
      "outputs": [],
      "source": [
        "def clean_usa(usa,measure_column, min_date, max_date,divied_s,trend,group_d):\n",
        "    usa_copy = usa.copy()\n",
        "    usa = usa.dropna(subset=[measure_column])\n",
        "    usa['date'] = pd.to_datetime(usa['date'])\n",
        "    usa_filtered = usa[(usa['date'] >= min_date) & (usa['date'] <= max_date)]\n",
        "    usa_filtered.set_index('date', inplace=True)\n",
        "    numeric_columns = usa_filtered.select_dtypes(include=[np.number]).columns\n",
        "    usa_weekly = usa_filtered[numeric_columns].resample(group_d).mean()\n",
        "    usa_weekly_std = usa_filtered[numeric_columns].resample(group_d).std()\n",
        "\n",
        "\n",
        "    return usa_weekly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84485ad-0c57-4a4e-93ec-0239b29bb274",
      "metadata": {
        "id": "b84485ad-0c57-4a4e-93ec-0239b29bb274"
      },
      "outputs": [],
      "source": [
        "def excel_corr_data(usa,min_date, max_date,group_d,path,lags,divides_std,trend):\n",
        "    path_img = path +\"/imgs_corr_only_data_web\"\n",
        "    if not os.path.exists(path_img):\n",
        "        # If the directory does not exist, create it\n",
        "        os.makedirs(path_img)\n",
        "\n",
        "    measures = [ 'new_cases','hosp_patients', 'new_deaths']#'hosp_patients_smoothed',\n",
        "\n",
        "    # Initialize list to store results\n",
        "    table_data = []\n",
        "\n",
        "    for measure_1 in measures:\n",
        "        for measure_2 in measures:\n",
        "            if measure_2==measure_1:\n",
        "                continue\n",
        "\n",
        "        # Analyze correlations\n",
        "            result = analyze_measure_correlations_data_from_web(usa, measure_1, measure_2, f't', min_date, max_date,group_d,lags,divides_std,trend)\n",
        "            table_data.append([measure_2,measure_1,result[0],result[1],result[2]])\n",
        "            analyze_correlations_data_web_im(usa,measure_1, measure_2 , path_img,min_date,max_date,lags,divides_std,group_d,trend)\n",
        "        # Append results for healthcare professionals and non-healthcare professionals\n",
        "    columns = ['measure_1','measure_2','corr','significance',\"lag\"]\n",
        "    # Convert results to DataFrame\n",
        "    results_df_new = pd.DataFrame(table_data,columns=columns)\n",
        "    results_df_new.to_csv(path+\"/results_corr_finish_data_usa.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4276944-fe96-44ad-bca2-b510f017ac40",
      "metadata": {
        "id": "e4276944-fe96-44ad-bca2-b510f017ac40"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def analyze_correlations_between_em_me(usa, emotion_data_h, emotion_data_g, measure_column, output_path, min_date, max_date,time_d,lags,divides_std,trend):\n",
        "\n",
        "    emotion_data_h.index = pd.to_datetime(emotion_data_h.index)\n",
        "    emotion_data_g.index = pd.to_datetime(emotion_data_g.index)\n",
        "\n",
        "    # if divides_std:\n",
        "    #     usa_weekly = usa_weekly/usa_weekly_std\n",
        "\n",
        "    # Select the relevant columns\n",
        "    weekly_avg_daily_confirmed_cases = clean_usa(usa,measure_column, min_date, max_date,divides_std,trend,time_d)[measure_column]\n",
        "    aggregate_emotions_h_df = emotion_data_h[['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']]\n",
        "    aggregate_emotions_g_df = emotion_data_g[['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']]\n",
        "\n",
        "    # Align the datasets to ensure they have the same length and the same dates\n",
        "    aligned_data_h = pd.merge(weekly_avg_daily_confirmed_cases, aggregate_emotions_h_df, left_index=True, right_index=True, how='inner')\n",
        "    aligned_data_g = pd.merge(weekly_avg_daily_confirmed_cases, aggregate_emotions_g_df, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    # Define normalization function\n",
        "    def normalize(x, range=(0, 1)):\n",
        "        return (x - np.min(x)) / (np.max(x) - np.min(x)) * (range[1] - range[0]) + range[0]\n",
        "\n",
        "    # Define function to calculate Spearman correlation with lag\n",
        "    def spearmanr_with_lag(x, y, lag):\n",
        "        if lag > 0:\n",
        "            return spearmanr(x[:-lag], y[lag:])\n",
        "        elif lag < 0:\n",
        "            return spearmanr(x[-lag:], y[:lag])\n",
        "        else:\n",
        "            return spearmanr(x, y)\n",
        "\n",
        "    # Calculate and plot correlations for both populations on the same graph\n",
        "    emotion_list = ['avg_joy', 'avg_sadness', 'avg_anger', 'avg_surprise', 'avg_disgust', 'avg_fear']\n",
        "    measure_name = f'weekly_avg_{measure_column}'\n",
        "    results_data = []\n",
        "\n",
        "    for emotion in emotion_list:\n",
        "        # Normalize data\n",
        "\n",
        "\n",
        "\n",
        "        measure_data_h = aligned_data_h[measure_column].tolist()\n",
        "        emotion_data_normalized_h = aligned_data_h[emotion].tolist()\n",
        "        measure_data_g = aligned_data_g[measure_column].tolist()\n",
        "        emotion_data_normalized_g = aligned_data_g[emotion].tolist()\n",
        "        if trend:\n",
        "            stl = STL(emotion_data_normalized_h, seasonal=21).fit() #######################3new\n",
        "            emotion_data_normalized_h = stl.trend\n",
        "\n",
        "            stl = STL(emotion_data_normalized_g, seasonal=21).fit() #######################3new\n",
        "            emotion_data_normalized_g = stl.trend\n",
        "\n",
        "        corrs_h = []\n",
        "        pvalues_h = []\n",
        "        corrs_g = []\n",
        "        pvalues_g = []\n",
        "\n",
        "        for displacement in lags:\n",
        "            corr_h = spearmanr_with_lag(measure_data_h, emotion_data_normalized_h, displacement)\n",
        "            corrs_h.append(corr_h[0])\n",
        "            pvalues_h.append(corr_h[1])\n",
        "            corr_g = spearmanr_with_lag(measure_data_g, emotion_data_normalized_g, displacement)\n",
        "            corrs_g.append(corr_g[0])\n",
        "            pvalues_g.append(corr_g[1])\n",
        "            results_data.append([emotion, displacement, corr_h[0], corr_h[1], corr_g[0], corr_g[1]])\n",
        "\n",
        "\n",
        "        displacement = lags\n",
        "        # Plot correlations on the same graph\n",
        "        plt.figure(figsize=(30, 10))\n",
        "        added_h_marker = False\n",
        "        added_g_marker = False\n",
        "        for i, pval in enumerate(pvalues_h):\n",
        "            if pval < 0.05:\n",
        "                plt.plot(displacement[i], corrs_h[i], '^', color='red', markersize=15, label='p < 0.05 HCPs' if not added_h_marker else \"\")\n",
        "                added_h_marker = True\n",
        "\n",
        "        for i, pval in enumerate(pvalues_g):\n",
        "            if pval < 0.05:\n",
        "                plt.plot(displacement[i], corrs_g[i], '^', color='blue', markersize=15, label='p < 0.05 Non-HCPs' if not added_g_marker else \"\")\n",
        "                added_g_marker = True\n",
        "\n",
        "        plt.plot(displacement, corrs_h, label='HCPs')\n",
        "        plt.plot(displacement, corrs_g, label='Non-HCPs')\n",
        "\n",
        "        # Add Spearman correlation values for corrs_h and corrs_g\n",
        "        for i, (corr_h, corr_g) in enumerate(zip(corrs_h, corrs_g)):\n",
        "            plt.text(displacement[i], corrs_h[i], f'{corr_h:.3f}', fontsize=26, ha='center', color='black', verticalalignment='bottom')\n",
        "            plt.text(displacement[i], corrs_g[i], f'{corr_g:.3f}', fontsize=26, ha='center', color='black', verticalalignment='top')\n",
        "\n",
        "        plt.title(f'Spearman correlation between {emotion.replace(\"avg_\", \"\")} and {measure_name.replace(\"weekly_avg_\", \"\").replace(\"_\", \" \")}', fontsize=35)\n",
        "        plt.xlabel('Lags', fontsize=38)\n",
        "        plt.ylabel('correlation coefficient', fontsize=38)\n",
        "        # plt.legend(fontsize=16)  # Increase the font size of the legend\n",
        "\n",
        "        # Increase the font size of the X and Y tick labels\n",
        "        plt.xticks(fontsize=35)\n",
        "        plt.yticks(fontsize=35)\n",
        "        plt.legend(fontsize=22)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_path+ f'/correlation_between_{emotion}_and_{measure_name}.png')\n",
        "    results_df = pd.DataFrame(results_data, columns=['Emotion', 'Lag', 'Correlation_HC', 'P-Value_HC', 'Correlation_NonHC', 'P-Value_NonHC'])\n",
        "    results_df.to_excel(os.path.join(output_path, f'correlation{measure_column}_results.xlsx'), index=False)\n",
        "\n",
        "        # plt.show()\n",
        "\n",
        "# Example usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1d27ed-eae9-4b34-86b6-36628c821940",
      "metadata": {
        "id": "aa1d27ed-eae9-4b34-86b6-36628c821940"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def analyze_measure_correlations_data_from_web(usa, measure_column_1, measure_column_2, output_path, min_date, max_date,group_d,lags,divied_s,trend):\n",
        "\n",
        "\n",
        "    usa_weekly_1 = clean_usa(usa,measure_column_1, min_date, max_date,divied_s,trend,group_d)[measure_column_1]\n",
        "    usa_weekly_2 =  clean_usa(usa,measure_column_2, min_date, max_date,divied_s,trend,group_d)[measure_column_2]\n",
        "\n",
        "    # Align the datasets\n",
        "    aligned_data = pd.merge(usa_weekly_1, usa_weekly_2, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    # Normalize the data\n",
        "    def normalize(x):\n",
        "        return (x - x.min()) / (x.max() - x.min()) if x.max() != x.min() else x\n",
        "\n",
        "\n",
        "    # aligned_data[measure_column_1] = normalize(aligned_data[measure_column_1])\n",
        "    # aligned_data[measure_column_2] = normalize(aligned_data[measure_column_2])  !!!!!!!!!!!!!!!!!!!!!!\n",
        "    aligned_data[measure_column_1] = aligned_data[measure_column_1].tolist()\n",
        "    aligned_data[measure_column_2] = aligned_data[measure_column_2].tolist()\n",
        "\n",
        "    # Calculate Spearman correlation with lag\n",
        "    max_corr_h = 0  # For healthcare professionals\n",
        "    best_pvalue_h = 0\n",
        "    best_lag_h = 0\n",
        "    # lags = range(-8, 8)\n",
        "    for lag in lags:\n",
        "        if lag > 0:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1][:-lag], aligned_data[measure_column_2][lag:])\n",
        "        elif lag < 0:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1][-lag:], aligned_data[measure_column_2][:lag])\n",
        "        else:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1], aligned_data[measure_column_2])\n",
        "\n",
        "\n",
        "        if abs(max_corr_h)< abs(corr):\n",
        "                max_corr_h = corr\n",
        "                best_pvalue_h = pval\n",
        "                best_lag_h = lag\n",
        "\n",
        "\n",
        "\n",
        "    return max_corr_h,best_pvalue_h,best_lag_h\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38edbfe-174e-486d-87bb-bec1e96c52f2",
      "metadata": {
        "id": "b38edbfe-174e-486d-87bb-bec1e96c52f2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import pandas as pd\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "\n",
        "def graph_norm(overall_weekly_stats_h,overall_weekly_stats_g, measure,emotion,group_d,path,min_date,max_date,divied_s,trend,usa):\n",
        "    emotion_data_h = overall_weekly_stats_h\n",
        "\n",
        "    # Reset index for emotion data\n",
        "    emotion_data_h = emotion_data_h.reset_index()\n",
        "\n",
        "    emotion_data_g = overall_weekly_stats_g.reset_index()\n",
        "\n",
        "\n",
        "    usa_filtered = clean_usa(usa,measure, min_date, max_date,divied_s,trend,group_d)\n",
        "\n",
        "    usa_filtered = usa_filtered.reset_index()\n",
        "    # if measure == 'new_cases':\n",
        "    #     print(usa_filtered[measure])\n",
        "        # print(usa_weekly[measure])\n",
        "    def normalize(x):\n",
        "        return (x - x.min()) / (x.max() - x.min()) if x.max() != x.min() else x\n",
        "\n",
        "    usa_filtered[f'{measure}_norm'] = normalize(usa_filtered[measure])\n",
        "    emotion_data_h[f'avg_{emotion}_smooth'] = savgol_filter(emotion_data_h[f'avg_{emotion}'], window_length=11, polyorder=2)\n",
        "    emotion_data_g[f'avg_{emotion}_smooth'] = savgol_filter(emotion_data_g[f'avg_{emotion}'], window_length=11, polyorder=2)\n",
        "\n",
        "    emotion_data_h[f'avg_{emotion}_norm'] = normalize(emotion_data_h[f'avg_{emotion}_smooth']) #avg_fear !!!!!!!!!!!!!!!!!1\n",
        "    emotion_data_g[f'avg_{emotion}_norm'] = normalize(emotion_data_g[f'avg_{emotion}_smooth'])\n",
        "\n",
        "\n",
        "\n",
        "    if trend:\n",
        "        stl = STL(emotion_data_h[f'avg_{emotion}_norm'], seasonal=21).fit() #######################3new\n",
        "        emotion_data_h[f'avg_{emotion}_norm'] = stl.trend\n",
        "\n",
        "        stl = STL(emotion_data_g[f'avg_{emotion}_norm'], seasonal=21).fit() #######################3new\n",
        "        emotion_data_g[f'avg_{emotion}_norm'] = stl.trend\n",
        "    measure_1 = measure\n",
        "    plt.figure(figsize=(19, 6))\n",
        "    if measure== \"new_cases\":\n",
        "        measure_1 = \"New Cases\"\n",
        "\n",
        "    # Plotting the measure data\n",
        "    plt.plot(usa_filtered['date'], usa_filtered[f'{measure}_norm'], label=measure_1, color='red')\n",
        "    plt.fill_between(usa_filtered['date'], usa_filtered[f'{measure}_norm'], color='red', alpha=0.3)\n",
        "\n",
        "\n",
        "    # Plotting 'fear' data from emotion_data_h\n",
        "    plt.plot(emotion_data_h['date'], emotion_data_h[f'avg_{emotion}_norm'], label=f'HCPs', color='blue')\n",
        "    plt.plot(emotion_data_g['date'], emotion_data_g[f'avg_{emotion}_norm'], label=f'Non-HCPs', color='orange')\n",
        "       # Get the minimum and maximum date from the datasets and set the x-axis limits\n",
        "    min_date = min(emotion_data_h['date'].min(), emotion_data_g['date'].min())\n",
        "    max_date = max(emotion_data_h['date'].max(), emotion_data_g['date'].max())\n",
        "    plt.xlim(left=min_date, right=max_date)\n",
        "\n",
        "    # Setting the labels and title\n",
        "    plt.xlabel('Date', fontsize=22)\n",
        "    plt.ylabel('Normalized Emotion Score', fontsize=22)\n",
        "    if emotion== \"joy\":\n",
        "        emotion = \"Joy\"\n",
        "    if emotion == \"fear\":\n",
        "        emotion= \"Fear\"\n",
        "    if emotion == \"sadness\":\n",
        "        emotion= \"Sadness\"\n",
        "\n",
        "\n",
        "    plt.title(f'{measure_1.replace(\"_\",\" \")} VS. {emotion}', fontsize=20)\n",
        "\n",
        "    # Adding grid and legend\n",
        "    plt.grid(False)\n",
        "    # plt.legend()\n",
        "\n",
        "    # Set major ticks format and interval for the X-axis\n",
        "    # plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Display a date every 3 months\n",
        "    # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format dates as 'YYYY-MM'\n",
        "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Display a date every week\n",
        "    # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%y-%m'))\n",
        "    plt.xticks(rotation=0,fontsize=14)\n",
        "\n",
        "    plt.yticks(fontsize=17)\n",
        "    plt.legend(fontsize=16,loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + f'/Comparison of COVID-19 {measure} and {emotion}.png')\n",
        "    # Display the plot\n",
        "    # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ac5a9b-8004-4630-a230-1100cfd7c5ab",
      "metadata": {
        "id": "b9ac5a9b-8004-4630-a230-1100cfd7c5ab"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def analyze_correlations_data_web_im(usa, measure_column_1, measure_column_2, output_path, min_date, max_date,lags,divied_s,group_d,trend):\n",
        "    usa_weekly_1 = clean_usa(usa,measure_column_1, min_date, max_date,divied_s,trend,group_d)[measure_column_1]\n",
        "    usa_weekly_2 = clean_usa(usa,measure_column_2, min_date, max_date,divied_s,trend,group_d)[measure_column_2]\n",
        "\n",
        "    # Align the datasets\n",
        "    aligned_data = pd.merge(usa_weekly_1, usa_weekly_2, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    # Normalize the data\n",
        "    def normalize(x):\n",
        "        return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "    # aligned_data[measure_column_1] = normalize(aligned_data[measure_column_1])   !!!!!!!!!!!!!!!!!!!!!!!!!!!!11\n",
        "    # aligned_data[measure_column_2] = normalize(aligned_data[measure_column_2])\n",
        "\n",
        "    aligned_data[measure_column_1] = aligned_data[measure_column_1].tolist()\n",
        "    aligned_data[measure_column_2] = aligned_data[measure_column_2].tolist()\n",
        "\n",
        "    # Calculate Spearman correlation with lag\n",
        "    corrs = []\n",
        "    pvalues = []\n",
        "    # lags = range(-8, 9)\n",
        "    for lag in lags:\n",
        "        if lag > 0:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1][:-lag], aligned_data[measure_column_2][lag:])\n",
        "        elif lag < 0:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1][-lag:], aligned_data[measure_column_2][:lag])\n",
        "        else:\n",
        "            corr, pval = spearmanr(aligned_data[measure_column_1], aligned_data[measure_column_2])\n",
        "        corrs.append(corr)\n",
        "        pvalues.append(pval)\n",
        "\n",
        "    # Plot Spearman correlation results with conditional triangles\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.plot(lags, corrs, label='Spearman Correlation')\n",
        "    # Adding triangles for significant correlations\n",
        "    for i, pval in enumerate(pvalues):\n",
        "        if pval < 0.05:\n",
        "            ax.plot(lags[i], corrs[i], '^', color='red', markersize=10, label='p < 0.05' if i == 0 else \"\")  # Add label only once\n",
        "\n",
        "    ax.set_title(f'Spearman correlation between {measure_column_1} and {measure_column_2}')\n",
        "    ax.set_xlabel('Lags',fontsize=17)\n",
        "    ax.set_ylabel('correlation coefficient',fontsize=17)\n",
        "    plt.yticks(fontsize=15)\n",
        "    plt.xticks(fontsize=15)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path+ f'/{measure_column_2}_{measure_column_1}.png')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d48f5c3-69b9-4427-81e3-9cd5150ff99f",
      "metadata": {
        "id": "9d48f5c3-69b9-4427-81e3-9cd5150ff99f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def graph_norm_1(overall_weekly_stats_h,overall_weekly_stats_g, measure,emotion,group_d,path,min_date,max_date,divied_s,trend,usa):\n",
        "    emotion_data_h = overall_weekly_stats_h\n",
        "\n",
        "    # Reset index for emotion data\n",
        "    emotion_data_h = emotion_data_h.reset_index()\n",
        "\n",
        "    emotion_data_g = overall_weekly_stats_g.reset_index()\n",
        "\n",
        "\n",
        "    usa_filtered = clean_usa(usa,measure, min_date, max_date,divied_s,trend,group_d)\n",
        "\n",
        "    usa_filtered = usa_filtered.reset_index()\n",
        "    # if measure == 'new_cases':\n",
        "    #     print(usa_filtered[measure])\n",
        "        # print(usa_weekly[measure])\n",
        "    def normalize(x):\n",
        "        return (x - x.min()) / (x.max() - x.min()) if x.max() != x.min() else x\n",
        "\n",
        "    usa_filtered[f'{measure}_norm'] = normalize(usa_filtered[measure])\n",
        "    emotion_data_h[f'avg_{emotion}_smooth'] = savgol_filter(emotion_data_h[f'avg_{emotion}'], window_length=11, polyorder=2)\n",
        "    emotion_data_g[f'avg_{emotion}_smooth'] = savgol_filter(emotion_data_g[f'avg_{emotion}'], window_length=11, polyorder=2)\n",
        "\n",
        "    emotion_data_h[f'avg_{emotion}_norm'] = normalize(emotion_data_h[f'avg_{emotion}_smooth']) #avg_fear !!!!!!!!!!!!!!!!!1\n",
        "    emotion_data_g[f'avg_{emotion}_norm'] = normalize(emotion_data_g[f'avg_{emotion}_smooth'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if trend:\n",
        "        stl = STL(emotion_data_h[f'avg_{emotion}_norm'], seasonal=21).fit() #######################3new\n",
        "        emotion_data_h[f'avg_{emotion}_norm'] = stl.trend\n",
        "\n",
        "        stl = STL(emotion_data_g[f'avg_{emotion}_norm'], seasonal=21).fit() #######################3new\n",
        "        emotion_data_g[f'avg_{emotion}_norm'] = stl.trend\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # Plotting the measure data\n",
        "    plt.plot(usa_filtered['date'], usa_filtered[f'{measure}_norm'], label=measure, color='red')\n",
        "    plt.fill_between(usa_filtered['date'], usa_filtered[f'{measure}_norm'], color='red', alpha=0.3)\n",
        "\n",
        "\n",
        "    # Plotting 'fear' data from emotion_data_h\n",
        "    plt.plot(emotion_data_h['date'], emotion_data_h[f'avg_{emotion}_norm'], label=f'HCPs', color='blue')\n",
        "    # plt.plot(emotion_data_g['date'], emotion_data_g[f'avg_{emotion}_norm'], label=f'Non-HCPs', color='orange')\n",
        "\n",
        "    # Setting the labels and title\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(f'Comparison of COVID-19 {measure} and {emotion}')\n",
        "\n",
        "    # Adding grid and legend\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Set major ticks format and interval for the X-axis\n",
        "    # plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Display a date every 3 months\n",
        "    # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format dates as 'YYYY-MM'\n",
        "    plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))  # Display a date every week\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path + f'/Comparison of COVID-19 {measure} and {emotion}_only_hcp.png')\n",
        "    # Display the plot\n",
        "    # plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c0402c-a6ea-4e9d-a31b-0154484ec861",
      "metadata": {
        "id": "91c0402c-a6ea-4e9d-a31b-0154484ec861"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0cdff6f-d487-4c64-8918-300b3823513b",
      "metadata": {
        "id": "c0cdff6f-d487-4c64-8918-300b3823513b"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# # Load HCP weekly stats\n",
        "# with open(\"Fig3/weekly_stats_data_HCPs.pkl\", \"rb\") as f:\n",
        "#     overall_weekly_stats_h = pickle.load(f)\n",
        "\n",
        "# # Load Non-HCP weekly stats\n",
        "# with open(\"Fig3/weekly_stats_data_NonHCPs.pkl\", \"rb\") as f:\n",
        "#     overall_weekly_stats_g = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_weekly_stats_h = pd.read_csv(\"weekly_stats_data_HCPs.csv\", index_col=\"date\", parse_dates=True)\n",
        "overall_weekly_stats_g = pd.read_csv(\"weekly_stats_data_NonHCPs.csv\", index_col=\"date\", parse_dates=True)\n"
      ],
      "metadata": {
        "id": "9_f0TKROSJLm"
      },
      "id": "9_f0TKROSJLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a5752e9",
      "metadata": {
        "id": "4a5752e9"
      },
      "outputs": [],
      "source": [
        "pipline_analysis(final_h,final_df,usa_filtered,\"review_hcw_1\", 'w',range(-8,9),False,False)#range(-56,57) (-8,9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec95d48-99d4-4389-8d54-a589fbf762dd",
      "metadata": {
        "id": "8ec95d48-99d4-4389-8d54-a589fbf762dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "653c8771-aad7-49c5-ba36-c5b6461acad1",
      "metadata": {
        "id": "653c8771-aad7-49c5-ba36-c5b6461acad1"
      },
      "outputs": [],
      "source": [
        "cases = pd.read_csv('data_covid/new_cases.csv')\n",
        "cases = cases[['date','United States']]\n",
        "deaths = pd.read_csv('data_covid/new_deaths.csv')\n",
        "deaths = deaths[['date','United States']]\n",
        "hosp = pd.read_csv('data_covid/covid-hospitalizations.csv')\n",
        "hosp_usa= hosp[hosp['iso_code'] == 'USA']\n",
        "hosp_usa = hosp_usa[hosp_usa['indicator']=='Weekly new hospital admissions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94906314",
      "metadata": {
        "id": "94906314"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize the input series by scaling between its min and max values.\n",
        "    \"\"\"\n",
        "    return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "# Load data\n",
        "cases = pd.read_csv('data_covid/new_cases.csv')\n",
        "cases = cases[['date', 'United States']]\n",
        "deaths = pd.read_csv('data_covid/new_deaths.csv')\n",
        "deaths = deaths[['date', 'United States']]\n",
        "hosp = pd.read_csv('data_covid/covid-hospitalizations.csv')\n",
        "\n",
        "# Filter data for the USA\n",
        "hosp_usa = hosp[(hosp['iso_code'] == 'USA') & (hosp['indicator'] == 'Weekly new hospital admissions')]\n",
        "\n",
        "# Convert date column to datetime\n",
        "cases['date'] = pd.to_datetime(cases['date'])\n",
        "deaths['date'] = pd.to_datetime(deaths['date'])\n",
        "hosp_usa['date'] = pd.to_datetime(hosp_usa['date'])\n",
        "\n",
        "# Merge datasets on date\n",
        "merged_df = pd.merge(cases, deaths, on='date', suffixes=('_cases', '_deaths'))\n",
        "merged_df = pd.merge(merged_df, hosp_usa[['date', 'value']], on='date')\n",
        "\n",
        "# Rename columns for easier reference\n",
        "merged_df.rename(columns={'United States_cases': 'new_cases',\n",
        "                          'United States_deaths': 'new_deaths',\n",
        "                          'value': 'new_hospital_admissions'}, inplace=True)\n",
        "\n",
        "# Normalize the data\n",
        "merged_df['normalized_new_cases'] = normalize(merged_df['new_cases'])\n",
        "merged_df['normalized_new_deaths'] = normalize(merged_df['new_deaths'])\n",
        "merged_df['normalized_new_hospital_admissions'] = normalize(merged_df['new_hospital_admissions'])\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(merged_df['date'], merged_df['normalized_new_cases'], label='New Cases', color='green')\n",
        "# plt.plot(merged_df['date'], merged_df['normalized_new_deaths'], label='New Deaths', color='blue')\n",
        "plt.plot(merged_df['date'], merged_df['normalized_new_hospital_admissions'], label='New Hospital Admissions', color='red')\n",
        "plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=4))  # Display a date every 4 weeks\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Normalized Data')\n",
        "plt.title('COVID-19 New Cases, Deaths, and Hospital Admissions in the United States')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Setting the date format on x-axis\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09650ea",
      "metadata": {
        "id": "b09650ea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize the input series by scaling between its min and max values.\n",
        "    \"\"\"\n",
        "    return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "# Load data\n",
        "cases = pd.read_csv('data_covid/new_cases.csv')\n",
        "cases = cases[['date', 'United States']]\n",
        "deaths = pd.read_csv('data_covid/new_deaths.csv')\n",
        "deaths = deaths[['date', 'United States']]\n",
        "hosp = pd.read_csv('data_covid/covid-hospitalizations.csv')\n",
        "\n",
        "# Filter data for the USA\n",
        "hosp_usa = hosp[(hosp['iso_code'] == 'USA') & (hosp['indicator'] == 'Weekly new hospital admissions')]\n",
        "\n",
        "# Convert date column to datetime\n",
        "cases['date'] = pd.to_datetime(cases['date'])\n",
        "deaths['date'] = pd.to_datetime(deaths['date'])\n",
        "hosp_usa['date'] = pd.to_datetime(hosp_usa['date'])\n",
        "\n",
        "# Merge datasets on date\n",
        "merged_df = pd.merge(cases, deaths, on='date', suffixes=('_cases', '_deaths'))\n",
        "merged_df = pd.merge(merged_df, hosp_usa[['date', 'value']], on='date', how='left')\n",
        "\n",
        "# Rename columns for easier reference\n",
        "merged_df.rename(columns={'United States_cases': 'new_cases',\n",
        "                          'United States_deaths': 'new_deaths',\n",
        "                          'value': 'new_hospital_admissions'}, inplace=True)\n",
        "\n",
        "merged_df = merged_df[merged_df['date'] <= pd.to_datetime('2022-05-01')]\n",
        "\n",
        "# Fill only the missing values in the middle with linear interpolation\n",
        "if pd.isna(merged_df['new_hospital_admissions'].iloc[0]):\n",
        "    merged_df['new_hospital_admissions'].iloc[0] = 0\n",
        "\n",
        "# merged_df['new_hospital_admissions'] = merged_df['new_hospital_admissions'].interpolate(method='spline', order=3)\n",
        "\n",
        "# Normalize the data\n",
        "merged_df['normalized_new_cases'] = normalize(merged_df['new_cases'])\n",
        "merged_df['normalized_new_deaths'] = normalize(merged_df['new_deaths'])\n",
        "merged_df['normalized_new_hospital_admissions'] = normalize(merged_df['new_hospital_admissions'])\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(merged_df['date'], merged_df['normalized_new_cases'], label='New Cases', color='green')\n",
        "plt.plot(merged_df['date'], merged_df['normalized_new_deaths'], label='New Deaths', color='blue')\n",
        "plt.plot(merged_df['date'], merged_df['normalized_new_hospital_admissions'], label='New Hospital Admissions', color='red')\n",
        "plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=4))  # Display a date every 4 weeks\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Normalized Data')\n",
        "plt.title('COVID-19 New Cases, Deaths, and Hospital Admissions in the United States')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Setting the date format on x-axis\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mmat_new",
      "language": "python",
      "name": "mmat"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}