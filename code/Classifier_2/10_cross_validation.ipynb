{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80e4fb0-9d21-444b-bdf4-fb7d0fd05fe6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5af406-368e-4085-8c0f-568d2b1ad307",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "healthcare_workers_df = pd.read_csv('hpc_train_1529.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd154ca9-747b-44fe-a6cc-49243399784e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target_field = \"Occupation_Type (HCP/Not HCP)\"\n",
    "mapping_dict = {\n",
    "    \"Hcp\": 1,\n",
    "    \"Not hcp\": 0\n",
    "}\n",
    "inverse_mapping_dict = {v: k for k, v in mapping_dict.items()}\n",
    "healthcare_workers_df['author_type_numeric'] = healthcare_workers_df[target_field].map(mapping_dict)\n",
    "healthcare_workers_df['author_full_name_and_description'] = healthcare_workers_df['author_full_name'].str.cat(healthcare_workers_df['description'],sep=\" \")\n",
    "healthcare_workers_df = healthcare_workers_df[(healthcare_workers_df['Occupation_Type (HCP/Not HCP)'] == 'Hcp') | (healthcare_workers_df['Occupation_Type (HCP/Not HCP)'] == 'Not hcp') ]\n",
    "healthcare_workers_df = healthcare_workers_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656bb67-3243-4243-8866-aa1954c45b59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Separate the instances for each class\n",
    "df_class_0 = healthcare_workers_df.loc[healthcare_workers_df['author_type_numeric'] == 0]\n",
    "df_class_1 = healthcare_workers_df.loc[healthcare_workers_df['author_type_numeric'] == 1]\n",
    "\n",
    "# Get the number of instances for each class\n",
    "count_class_0, count_class_1 = df_class_0.shape[0], df_class_1.shape[0]\n",
    "\n",
    "# Determine the size of the subset to select from the majority class\n",
    "subset_size = count_class_0\n",
    "\n",
    "# Sample a subset of instances from the majority class\n",
    "df_class_1_subset = df_class_1.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Concatenate the subset of instances from the majority class with all instances from the minority class\n",
    "healthcare_workers_balanced_df = pd.concat([df_class_1_subset, df_class_0], axis=0)\n",
    "\n",
    "# Shuffle the dataframe to mix the instances of each class\n",
    "healthcare_workers_balanced_df = healthcare_workers_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "healthcare_workers_balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce3d77-fa40-4606-8349-ad73e5478aaa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "# test = .sample(15)\n",
    "# test.reset_index(drop=True, inplace=True)\n",
    "X = healthcare_workers_balanced_df['author_full_name_and_description']\n",
    "y = healthcare_workers_balanced_df['author_type_numeric']\n",
    "\n",
    "# Loop through each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "# Set up cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"Training on fold {fold_idx}...\")\n",
    "    \n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "    # Optional model configuration\n",
    "    model_args = ClassificationArgs(num_train_epochs=1,overwrite_output_dir=True)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = ClassificationModel(\n",
    "      \"roberta\", \"roberta-base\", args=model_args, use_cuda=False\n",
    "    )\n",
    "\n",
    "    # Train the model on the training set for this fold\n",
    "    train_df = pd.DataFrame({\"text\": X_train, \"labels\": y_train})\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    # Evaluate the model on the test set for this fold\n",
    "    eval_df = pd.DataFrame({\"text\": X_test, \"labels\": y_test})\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    y_true = eval_df[\"labels\"]\n",
    "    y_pred = np.argmax(model_outputs, axis=1)\n",
    "    accuracy = (result['tp'] + result['tn']) / (result['tp'] + result['tn'] + result['fp'] + result['fn'])\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Accuracy on fold {fold_idx}: {accuracy:.2f}\")\n",
    "    print(f\"Precision on fold {fold_idx}: {precision:.2f}\")\n",
    "    print(f\"Recall on fold {fold_idx}: {recall:.2f}\")\n",
    "    print(f\"F1-score on fold {fold_idx}: {f1:.2f}\")\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'accuracy': accuracy_scores,\n",
    "        'precision': precision_scores,\n",
    "        'recall': recall_scores,\n",
    "        'f1_score': f1_scores\n",
    "    }).to_csv('results_1.csv')\n",
    "\n",
    "# Print the average evaluation metrics over all folds\n",
    "print(f\"Average accuracy: {sum(accuracy_scores) / len(accuracy_scores):.2f}\")\n",
    "print(f\"Average precision: {sum(precision_scores) / len(precision_scores):.2f}\")\n",
    "print(f\"Average recall: {sum(recall_scores) / len(recall_scores):.2f}\")\n",
    "print(f\"Average F1-score: {sum(f1_scores) / len(f1_scores):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}